{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b10264",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0a07d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils.names import get_file_names, get_model_names\n",
    "\n",
    "files = get_file_names()\n",
    "models = get_model_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cab0b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtitle():\n",
    "    def __init__(self, start, end, text):        \n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e219d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(predictions, json_path):\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(predictions, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def save_text_file(transcriptions, path):\n",
    "    hp_text = \"\"\n",
    "    for subtitle in transcriptions:\n",
    "        hp_text += f\"{subtitle.text} \"\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write(hp_text)\n",
    "\n",
    "def save_text_shift_file(transcriptions, path):\n",
    "    hp_text = \"\"\n",
    "    for subtitle in transcriptions:\n",
    "        hp_text += f\"{subtitle.text}\\n\"\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write(hp_text)\n",
    "\n",
    "def ms_to_srt_time(ms):\n",
    "    hours = ms // 3600000\n",
    "    minutes = (ms % 3600000) // 60000\n",
    "    seconds = (ms % 60000) // 1000\n",
    "    milliseconds = ms % 1000\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}\"\n",
    "\n",
    "\n",
    "def save_srt_file(transcriptions, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for id, subtitle in enumerate(transcriptions):\n",
    "            f.write(f\"{id}\\n\")\n",
    "            f.write(f\"{ms_to_srt_time(subtitle.start)} --> {ms_to_srt_time(subtitle.end)}\\n\")\n",
    "            f.write(f\"{subtitle.text}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bfcb26",
   "metadata": {},
   "source": [
    "# Preprocess Whisper predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bd9215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_whisper_json(predictions):\n",
    "    transcriptions = []\n",
    "\n",
    "    # iterate over all segments\n",
    "    for prediction in predictions['predictions']:                \n",
    "        for segment in prediction[\"result\"]:                    \n",
    "            start_sec = segment[\"start\"]\n",
    "            end_sec = segment[\"end\"]\n",
    "\n",
    "            transcriptions.append(\n",
    "                Subtitle(                    \n",
    "                    text=segment[\"text\"].strip(),                            \n",
    "                    start=int(start_sec * 1000),\n",
    "                    end=int(end_sec * 1000)              \n",
    "                )\n",
    "            )\n",
    "    return transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d002a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    for file in files:\n",
    "        json_path = f\"../data/{model}/json/{file}.json\"\n",
    "        text_path = f\"../data/{model}/text/{file}.txt\"\n",
    "        srt_path = f\"../data/{model}/srt/{file}.srt\"\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            predictions = json.load(f)        \n",
    "        try:\n",
    "            if (\n",
    "                \"predictions\" in predictions and\n",
    "                isinstance(predictions[\"predictions\"], list) and\n",
    "                len(predictions[\"predictions\"]) > 0 and\n",
    "                \"result\" in predictions[\"predictions\"][0]\n",
    "            ):\n",
    "                save_json(predictions[\"predictions\"][0]['result'],json_path)\n",
    "                predictions = process_whisper_json(predictions)        \n",
    "                save_text_file(predictions, text_path)                \n",
    "                save_srt_file(predictions, srt_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Check {json_path} because i didn't success in saving\")  \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a177d",
   "metadata": {},
   "source": [
    "# Preprocess AssemblyAI predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6f0ef",
   "metadata": {},
   "source": [
    "AssemblyAI gives us the times in milliseconds: need to change to second to be standard with whisper and parakeet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b821487",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "        assemblyai_path = f\"../data/assemblyai/json/{file}.json\"\n",
    "\n",
    "        with open(assemblyai_path, 'r', encoding='utf-8') as f:\n",
    "                predictions = json.load(f)  \n",
    "\n",
    "        for segment in predictions:\n",
    "                segment[\"start\"] = segment[\"start\"]/1000\n",
    "                segment[\"end\"] = segment[\"end\"]/1000\n",
    "                for word in segment[\"words\"]:\n",
    "                        word[\"start\"] = word[\"start\"]/1000\n",
    "                        word[\"end\"] = word[\"end\"]/1000\n",
    "\n",
    "        with open(assemblyai_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(predictions, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b56ca",
   "metadata": {},
   "source": [
    "# Preprocess Parakeet predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18440782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_parakeet_json(predictions):\n",
    "    transcriptions = []\n",
    "\n",
    "    # iterate over all segments\n",
    "    for segment in predictions:                       \n",
    "        start_sec = segment[\"start\"]\n",
    "        end_sec = segment[\"end\"]\n",
    "\n",
    "        transcriptions.append(\n",
    "            Subtitle(                    \n",
    "                text=segment[\"text\"].strip(),                            \n",
    "                start=int(start_sec * 1000),\n",
    "                end=int(end_sec * 1000)              \n",
    "            )\n",
    "        )\n",
    "    return transcriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37fb1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    json_path = f\"../data/parakeet/json/{file}.json\"\n",
    "    text_path = f\"../data/parakeet/text/{file}.txt\"\n",
    "    srt_path = f\"../data/parakeet/srt/{file}.srt\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        predictions = json.load(f)        \n",
    "    try:            \n",
    "        predictions = process_parakeet_json(predictions)        \n",
    "        save_text_file(predictions, text_path)\n",
    "        save_srt_file(predictions, srt_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Check {json_path} because i didn't success in saving: {e}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2031c8f1",
   "metadata": {},
   "source": [
    "# Exploration of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982f8b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricati 120 file SRT\n",
      "Esempio: primo modello = parakeet primo file = MEZZORAINPIU_10_10_21.srt, numero segmenti = 317\n"
     ]
    }
   ],
   "source": [
    "from standardization.standardization_utils import load_all_subtitles\n",
    "\n",
    "srt_directories = [f\"../data/{model}/srt\" for model in models]\n",
    "\n",
    "\n",
    "all_subtitles = load_all_subtitles(srt_directories)\n",
    "\n",
    "\n",
    "# Verifica esempio:\n",
    "print(f\"Caricati {len(all_subtitles)} file SRT\")\n",
    "print(f\"Esempio: primo modello = {all_subtitles[0][0]} primo file = {all_subtitles[0][1]}, numero segmenti = {len(all_subtitles[0][2])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
