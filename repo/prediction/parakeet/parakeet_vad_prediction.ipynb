{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94568f1c",
   "metadata": {},
   "source": [
    "### Prediction with NVIDIA Parakeet TDT 0.6B model\n",
    "\n",
    "With VAD preprocessing to avoid CUDA out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8209ac7-306d-4dae-87b5-4f6c1c4aa1b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "def download_gcs_file(gcs_uri: str, dest_path=None) -> str:    \n",
    "    try:\n",
    "        if not gcs_uri.startswith(\"gs://\"):\n",
    "            raise ValueError(\"URI non valido, deve iniziare con 'gs://'\")\n",
    "        bucket_name, blob_name = gcs_uri.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        suffix = os.path.splitext(blob_name)[1]\n",
    "        if dest_path:\n",
    "            blob.download_to_filename(dest_path)\n",
    "            return dest_path\n",
    "        else:\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:\n",
    "                blob.download_to_filename(tmp_file.name)\n",
    "                print(f\"File scaricato da {gcs_uri} a {tmp_file.name}\")\n",
    "                return tmp_file.name\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Errore durante il download da GCS {gcs_uri}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cbcb86a-4dea-4474-ac37-a77ba8fb6b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4274a8c-c971-428a-8432-68ba130991eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "import tempfile\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "import nemo.collections.asr as nemo_asr\n",
    "\n",
    "# Pipeline per fare predizione con il modello Nvidia Parakeet 0.6 B con VAD preprocessing e gestione resiliente\n",
    "\n",
    "def _deterministic_cache_dir(audio_path: str, cache_root: str = None) -> str:\n",
    "    \"\"\"Crea una cartella cache deterministica basata sul nome file audio.\n",
    "    Esempio: /tmp/vad_cache/<basename_senza_ext>\"\"\"\n",
    "    base = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "    cache_root = cache_root or os.path.join(tempfile.gettempdir(), \"vad_cache\")\n",
    "    os.makedirs(cache_root, exist_ok=True)\n",
    "    cache_dir = os.path.join(cache_root, base)\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    return cache_dir\n",
    "\n",
    "\n",
    "def _run_vad_and_save_manifest(\n",
    "    audio_path: str,\n",
    "    cache_dir: str,\n",
    "    sample_rate: int = 16000,\n",
    "    max_duration: float = 30.0,\n",
    "    hf_token: str | None = None,\n",
    ") -> List[Tuple[float, float]]:\n",
    "    \"\"\"Esegue la VAD con pyannote, unisce i segmenti fino a max_duration e salva manifest su JSON.\n",
    "    Restituisce la lista dei segmenti uniti [(start, end), ...].\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(\"[INFO] Caricamento modello VAD pyannote.audio...\")\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/voice-activity-detection\",\n",
    "        use_auth_token=hf_token,\n",
    "    )\n",
    "    pipeline.to(device)\n",
    "\n",
    "    print(f\"[INFO] Analisi VAD in corso su {audio_path} ...\")\n",
    "    vad_output = pipeline(audio_path)\n",
    "\n",
    "    # segmenti grezzi\n",
    "    raw_segments = [(speech.start, speech.end) for speech in vad_output.get_timeline()]\n",
    "\n",
    "    # salva manifest per ripresa senza ripetere VAD\n",
    "    manifest_path = os.path.join(cache_dir, \"segments.json\")\n",
    "    meta = {\n",
    "        \"audio_path\": audio_path,\n",
    "        \"sample_rate\": sample_rate,\n",
    "        \"max_duration\": max_duration,\n",
    "        \"segments\": raw_segments,\n",
    "        \"created_at\": time.time(),\n",
    "    }\n",
    "    with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[SAVE] Manifest VAD: {manifest_path} ({len(raw_segments)} segmenti)\")\n",
    "    return raw_segments\n",
    "\n",
    "\n",
    "def prepare_vad_cache(\n",
    "    audio_path: str,\n",
    "    cache_root: str | None = None,\n",
    "    sample_rate: int = 16000,\n",
    "    max_duration: float = 30.0,\n",
    "    hf_token: str | None = None,\n",
    ") -> Tuple[List[Tuple[float, float]], str]:\n",
    "    \"\"\"Prepara (o carica) i segmenti VAD per un file audio.\n",
    "    - Se esiste segments.json, lo carica e *non* riesegue la VAD.\n",
    "    - Altrimenti esegue la VAD, salva il manifest e restituisce i segmenti.\n",
    "\n",
    "    Ritorna: (segments, cache_dir)\n",
    "    \"\"\"\n",
    "    cache_dir = _deterministic_cache_dir(audio_path, cache_root)\n",
    "    manifest_path = os.path.join(cache_dir, \"segments.json\")\n",
    "\n",
    "    if os.path.exists(manifest_path):\n",
    "        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            meta = json.load(f)\n",
    "        segments = [tuple(seg) for seg in meta[\"segments\"]]\n",
    "        print(f\"[INFO] Manifest VAD trovato: {manifest_path} ({len(segments)} segmenti)\")\n",
    "    else:\n",
    "        # token da env (se non passato esplicitamente)\n",
    "        token = \"mytoken\"\n",
    "        segments = _run_vad_and_save_manifest(\n",
    "            audio_path,\n",
    "            cache_dir,\n",
    "            sample_rate=sample_rate,\n",
    "            max_duration=max_duration,\n",
    "            hf_token=token,\n",
    "        )\n",
    "    return segments, cache_dir\n",
    "\n",
    "\n",
    "def _export_chunks_for_indices(\n",
    "    audio_path: str,\n",
    "    segments: List[Tuple[float, float]],\n",
    "    indices: List[int],\n",
    "    cache_dir: str,\n",
    "    sample_rate: int = 16000,\n",
    ") -> List[str]:\n",
    "    \"\"\"Esporta su disco i chunk WAV solo per gli indici richiesti e restituisce i path.\n",
    "    I file sono creati come {cache_dir}/chunk_XXX.wav\n",
    "    \"\"\"\n",
    "    waveform, orig_sr = torchaudio.load(audio_path)\n",
    "    # mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # resampler (solo se necessario)\n",
    "    resampler = None\n",
    "    if orig_sr != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_sr, sample_rate)\n",
    "\n",
    "    out_paths = []\n",
    "    for idx in indices:\n",
    "        s, e = segments[idx]\n",
    "        start_sample = int(s * orig_sr)\n",
    "        end_sample = int(e * orig_sr)\n",
    "        chunk = waveform[:, start_sample:end_sample]\n",
    "        if resampler is not None:\n",
    "            chunk = resampler(chunk)\n",
    "        out_path = os.path.join(cache_dir, f\"chunk_{idx+1:03d}.wav\")\n",
    "        torchaudio.save(out_path, chunk, sample_rate)\n",
    "        out_paths.append(out_path)\n",
    "    return out_paths\n",
    "\n",
    "\n",
    "def _delete_files(paths: List[str]) -> None:\n",
    "    for p in paths:\n",
    "        try:\n",
    "            if os.path.exists(p):\n",
    "                os.remove(p)\n",
    "        except Exception as exc:\n",
    "            print(f\"[WARN] Impossibile rimuovere {p}: {exc}\")\n",
    "\n",
    "\n",
    "def _cleanup_cache_dir(cache_dir: str) -> None:\n",
    "    \"\"\"Rimuove completamente la cartella cache (chunk + manifest).\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(cache_dir):\n",
    "            shutil.rmtree(cache_dir)\n",
    "            print(f\"[CLEAN] Cache rimossa: {cache_dir}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"[WARN] Cleanup incompleto {cache_dir}: {exc}\")\n",
    "\n",
    "def transcribe_chunks_resumable(\n",
    "    audio_path: str,\n",
    "    segments: List[Tuple[float, float]],\n",
    "    cache_dir: str,\n",
    "    model_name: str,\n",
    "    output_segments_json: str,\n",
    "    output_words_json: str,\n",
    "    batch_size: int = 2,\n",
    "    sample_rate: int = 16000,\n",
    "    cleanup_on_success: bool = True,\n",
    ") -> Tuple[list, list, float]:\n",
    "    \"\"\"Trascrive i segmenti con ripresa, cleanup chunk per-batch e skip chunk falliti.\n",
    "\n",
    "    - Usa progress.json per sapere quanti chunk sono già stati completati.\n",
    "    - Esporta solo i chunk necessari per il batch corrente; dopo il salvataggio\n",
    "      dei risultati, elimina immediatamente i file WAV del batch.\n",
    "    - Se un chunk fallisce, viene saltato e registrato in skipped_chunks.json.\n",
    "    - Se l'intero job termina con successo, opzionalmente rimuove tutta la cache.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import time\n",
    "    import os\n",
    "\n",
    "    # Carica (o crea) file di output incrementali\n",
    "    if os.path.exists(output_segments_json):\n",
    "        with open(output_segments_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            segments_result = json.load(f)\n",
    "    else:\n",
    "        segments_result = []\n",
    "\n",
    "    if os.path.exists(output_words_json):\n",
    "        with open(output_words_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            words_result = json.load(f)\n",
    "    else:\n",
    "        words_result = []\n",
    "\n",
    "    # Stato di avanzamento\n",
    "    progress_path = os.path.join(cache_dir, \"progress.json\")\n",
    "    if os.path.exists(progress_path):\n",
    "        with open(progress_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            progress = json.load(f)\n",
    "        processed_chunks = int(progress.get(\"processed_chunks\", 0))\n",
    "    else:\n",
    "        processed_chunks = 0\n",
    "\n",
    "    skipped_path = os.path.join(cache_dir, \"skipped_chunks.json\")\n",
    "    if os.path.exists(skipped_path):\n",
    "        with open(skipped_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            skipped_chunks = json.load(f)\n",
    "    else:\n",
    "        skipped_chunks = []\n",
    "\n",
    "    total_chunks = len(segments)\n",
    "    print(f\"[INFO] Caricamento modello {model_name}...\")\n",
    "    asr_model = nemo_asr.models.ASRModel.from_pretrained(f\"nvidia/{model_name}\")\n",
    "\n",
    "    elapsed_time = 0.0\n",
    "    success = False\n",
    "\n",
    "    try:\n",
    "        for start_idx in range(processed_chunks, total_chunks, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, total_chunks)\n",
    "            batch_indices = list(range(start_idx, end_idx))\n",
    "\n",
    "            # Esporta chunk solo per questo batch\n",
    "            batch_paths = _export_chunks_for_indices(\n",
    "                audio_path, segments, batch_indices, cache_dir, sample_rate=sample_rate\n",
    "            )\n",
    "\n",
    "            print(f\"[ASR] Trascrizione batch {start_idx+1}-{end_idx}/{total_chunks}\")\n",
    "\n",
    "            for chunk_idx, chunk_path in zip(batch_indices, batch_paths):\n",
    "                t0 = time.time()\n",
    "                try:\n",
    "                    output = asr_model.transcribe([chunk_path], timestamps=True)[0]\n",
    "                    t1 = time.time()\n",
    "                    elapsed_time += (t1 - t0)\n",
    "\n",
    "                    offset_start, offset_end = segments[chunk_idx]\n",
    "                    # segment-level\n",
    "                    for seg in output.timestamp.get('segment', []):\n",
    "                        global_start = seg['start'] + offset_start\n",
    "                        global_end = seg['end'] + offset_start\n",
    "                        segments_result.append((global_start, global_end, seg['segment']))\n",
    "                    # word-level\n",
    "                    for w in output.timestamp.get('word', []):\n",
    "                        global_start = w['start'] + offset_start\n",
    "                        global_end = w['end'] + offset_start\n",
    "                        words_result.append((global_start, global_end, w['word']))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Trascrizione chunk {chunk_idx+1} fallita, skip: {e}\")\n",
    "                    skipped_chunks.append(chunk_idx)\n",
    "\n",
    "            # Salvataggio incrementale\n",
    "            with open(output_segments_json, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(segments_result, f, ensure_ascii=False, indent=2)\n",
    "            with open(output_words_json, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(words_result, f, ensure_ascii=False, indent=2)\n",
    "            with open(skipped_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(skipped_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            # Aggiorna progresso *dopo* il salvataggio\n",
    "            processed_chunks = end_idx\n",
    "            with open(progress_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\"processed_chunks\": processed_chunks}, f)\n",
    "\n",
    "            print(f\"[SAVE] Salvati progressi fino al chunk {end_idx}/{total_chunks}\")\n",
    "\n",
    "            # Cleanup immediato dei file WAV del batch\n",
    "            _delete_files(batch_paths)\n",
    "\n",
    "        success = True\n",
    "        print(\"\\n[INFO] Trascrizione completata.\")\n",
    "        print(f\"  Segmenti totali: {len(segments_result)}\")\n",
    "        print(f\"  Parole totali: {len(words_result)}\")\n",
    "        if skipped_chunks:\n",
    "            print(f\"  Chunk saltati: {skipped_chunks}\")\n",
    "        return segments_result, words_result, elapsed_time\n",
    "\n",
    "    finally:\n",
    "        # Pulizia se e solo se tutto è andato a buon fine\n",
    "        if success and cleanup_on_success:\n",
    "            _cleanup_cache_dir(cache_dir)\n",
    "        else:\n",
    "            print(f\"[INFO] Interruzione o errore: cache preservata in {cache_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed471309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(files: List[str]):\n",
    "    for file in files:\n",
    "        print(f\"\\n========== PROCESSING FILE {file} ==========\")\n",
    "        uri = f\"gs://bucket-stagisti/lucca_folder/audio/{file}\"\n",
    "        dest_path = f\"{file}\"\n",
    "        if not os.path.exists(dest_path):\n",
    "            local_uri = download_gcs_file(uri, dest_path=dest_path)\n",
    "        else:\n",
    "            local_uri = dest_path\n",
    "\n",
    "        # 1) Prepara (o ricarica) i segmenti VAD senza ripetere la VAD se già fatta\n",
    "        segments, cache_dir = prepare_vad_cache(\n",
    "            local_uri,\n",
    "            cache_root=None,            # default /tmp/vad_cache\n",
    "            sample_rate=16000,\n",
    "            max_duration=30.0,\n",
    "            hf_token=\"my_token\" \n",
    "        )\n",
    "\n",
    "        # 2) Trascrizione resiliente + cleanup automatico dei chunk\n",
    "        segments_result, words_result, elapsed_time = transcribe_chunks_resumable(\n",
    "            audio_path=local_uri,\n",
    "            segments=segments,\n",
    "            cache_dir=cache_dir,\n",
    "            model_name=\"parakeet-tdt-0.6b-v3\",\n",
    "            output_segments_json=f\"{file}_segments.json\",\n",
    "            output_words_json=f\"{file}_words.json\",\n",
    "            batch_size=8,\n",
    "            sample_rate=16000,\n",
    "            cleanup_on_success=True,   # elimina cache solo a job completato\n",
    "        )\n",
    "\n",
    "        with open(\"times.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{file} : {elapsed_time}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e69bcd-3e3a-4ae0-9942-a9df6ecdbfbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.names import get_file_names\n",
    "\n",
    "files = get_file_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd9ed28-cc8e-4495-ac43-ceeceee983a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_files(files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e69be86",
   "metadata": {},
   "source": [
    "### Formatting in single JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc813d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transcript_json(segments_result, words_result):\n",
    "    \"\"\"\n",
    "    Costruisce un JSON strutturato con segmenti e parole per ciascun segmento.\n",
    "\n",
    "    Args:\n",
    "        segments_result (list): lista di tuple (start, end, text)\n",
    "        words_result (list): lista di tuple (start, end, word, [score])\n",
    "\n",
    "    Returns:\n",
    "        list: transcript strutturato\n",
    "    \"\"\"\n",
    "    transcript = []\n",
    "\n",
    "    for seg_start, seg_end, seg_text in segments_result:\n",
    "        # Trova tutte le parole che appartengono al segmento\n",
    "        segment_words = [\n",
    "            {\n",
    "                \"word\": w[2],\n",
    "                \"start\": w[0],\n",
    "                \"end\": w[1]                \n",
    "            }\n",
    "            for w in words_result\n",
    "            if w[0] >= seg_start and w[1] <= seg_end\n",
    "        ]\n",
    "\n",
    "        transcript.append({\n",
    "            \"start\": seg_start,\n",
    "            \"end\": seg_end,\n",
    "            \"text\": seg_text,\n",
    "            \"words\": segment_words\n",
    "        })\n",
    "\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcac5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(f\"{file}_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        segments_result = json.load(f)\n",
    "    \n",
    "    with open(f\"{file}_words.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        words_result = json.load(f)\n",
    "    \n",
    "    final_json = build_transcript_json(segments_result, words_result)\n",
    "    \n",
    "    with open(f\"{file}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_json, f, ensure_ascii=False, indent=2)\n",
    "                                       \n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "nvidia_canary_env",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python (nvidia_canary_env) (Local)",
   "language": "python",
   "name": "nvidia_canary_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
